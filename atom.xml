<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大卞</title>
  <icon>https://www.gravatar.com/avatar/357d619055d419e9dcb96a2bddf8faec</icon>
  <subtitle>没有泪的夜晚,是天堂</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-05-05T14:05:48.123Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Spark Fly</name>
    <email>1994589823@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>19岁总结</title>
    <link href="http://example.com/undefined/35703.html"/>
    <id>http://example.com/undefined/35703.html</id>
    <published>2024-05-05T12:03:58.000Z</published>
    <updated>2024-05-05T14:05:48.123Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/image/001.jpg" alt="001"></p><p><strong>前情提要：</strong></p><p>想想之前上次生日还是订了个好大的冰淇淋蛋糕，但是那天下雨，还挺大的，但是不方便我觉得真的好好吃</p><h1 id="2023年-（大一下）"><a href="#2023年-（大一下）" class="headerlink" title="2023年 （大一下）"></a>2023年 （大一下）</h1><p><img src="/image/002.jpg" alt="002"></p><p>当时入党的时候去参观学习，感觉自己还是没变哈，只是好像脖子前倾还蛮严重的。</p><p><img src="/image/003.jpg" alt="003"></p><p>当时5.20的时候跑去西华去给天庆祝生日，真的没淋过这么大的雨，还只能在漏水的高架下面躲雨，乐。</p><p><img src="/image/004.jpg" alt="004"></p><p>早上还是蛮好的，夏天，天亮早，上早自习也好</p><p><img src="/image/005.jpg" alt="005"></p><p>终于见到医学院的鼠鼠了，感觉跟我一样。</p><p><img src="/image/006.jpg" alt="006"></p><p>说实话，臭美了一下，但是当时确实挺想用这个当导生照片的，哈哈，当时确实想过自己会当，还喊才才帮我摆拍了一下。</p><p><img src="/image/007.jpg" alt="007"></p><p>卞老师确实不喜欢在期末周剪头。</p><p><img src="/image/008.jpg" alt="008"></p><p>这衣服穿在卞某身上真的很好笑。</p><p><img src="/image/009.jpg" alt="009"></p><p>被迫因为衣服质量而当绣娘。</p><p><img src="/image/010.jpg" alt="010"></p><p>感谢小d当时给我写的捏。</p><p><img src="/image/011.jpg" alt="011"></p><p>好像是历史最低了。</p><p><img src="/image/012.jpg" alt="012"></p><p>出去和刘姐姐一起探讨八卦。</p><p><img src="/image/013.jpg" alt="013"></p><p>其实忍不住又烫了一下头发，但是好乱。</p><p><img src="/image/014.jpg" alt="014"></p><p>伤心的地方罢了。</p><p><img src="/image/015.jpg" alt="015"></p><p>lyy给我偷拍的，真的很没绷住。</p><h1 id="2023年（大二上）"><a href="#2023年（大二上）" class="headerlink" title="2023年（大二上）"></a>2023年（大二上）</h1><p><img src="/image/016.jpg" alt="016"></p><p><img src="/image/017.jpg" alt="017"></p><p><img src="/image/018.jpg" alt="018"></p><p><img src="/image/019.jpg" alt="019"></p><p>其实我是真的没有想到我能当导生，其实我还是觉得自己有时候不太合格，但是我只能说我尽力了，虽然比起来我自己可能有时候比较异想天开，或者不是一个很好的带头。甚至有时候我还在像一个小孩一样发癫，其实想想，我觉得还是蛮开心的，我也很感谢班上的孩子给我的包容和理解吧。</p><p><img src="/image/020.jpg" alt="020"></p><p>因为疫情，其实我没有真的入学过，这次也算是圆了我曾经的亏欠吧。</p><p><img src="/image/021.jpg" alt="021"></p><p>看看他们军训。</p><p><img src="/image/022.jpg" alt="022"></p><p>偷拍一下我们亲爱的迟少艺。</p><p><img src="/image/023.jpg" alt="023"></p><p>来听饶老师的组会了。</p><p><img src="/image/024.jpg" alt="024"></p><p><img src="/image/025.jpg" alt="025"></p><p>实验室里面的我和郭导。</p><p><img src="/image/026.jpg" alt="026"></p><p><img src="/image/027.jpg" alt="027"></p><p>从此开始迷上了学校的中医院，里面的按摩还是很舒服的。</p><p><img src="/image/028.jpg" alt="028"></p><p>跟迟哥学的（）。</p><p><img src="/image/029.jpg" alt="029"></p><p><img src="/image/030.jpg" alt="030"></p><p><img src="/image/031.jpg" alt="031"></p><p>这学期继续当酒鬼。我真是自己菜还喜欢喝。笑死，喝多了没少发疯。</p><p><img src="/image/032.jpg" alt="032"></p><p>国庆在学校待着，挺好看的。</p><p><img src="/image/033.jpg" alt="033"></p><p><img src="/image/034.jpg" alt="034"></p><p>好喜欢学校的猫猫，虽然三教的小猫已经不在这边了。</p><p><img src="/image/035.jpg" alt="035"></p><p>这学期自己还加了Viking，跟着柴哥做联邦学习。开心的是他们真是烟酒工作室，我好喜欢。经常和他们出去鬼混。</p><p><img src="/image/036.jpg" alt="036"></p><p>也是配好了自己的工位了，三教鼠鼠就是我。</p><p><img src="/image/037.jpg" alt="037"></p><p><img src="/image/038.jpg" alt="038"></p><p>主楼办公室我也好喜欢，还有镜子可以看看。（头发太长扎个辫子）</p><p><img src="/image/039.jpg" alt="039"></p><p>期末考试快完了，吃一下吧。</p><p><img src="/image/040.jpg" alt="040"></p><p>说实话，我看到这些的时候，我都在想，未来两年的我，还不是在这里一样，就像两年前的我，看着他们高三高考的感觉，一样的。我好害怕。</p><p><img src="/image/041.jpg" alt="041"></p><p>从此，我和孙算是和医学图像结下梁子了。PET-CT又爱又恨。</p><p><img src="/image/042.jpg" alt="042"></p><p>去看霉霉演唱会电影了，开心，全场都在唱。</p><h1 id="2024年（大二下）"><a href="#2024年（大二下）" class="headerlink" title="2024年（大二下）"></a>2024年（大二下）</h1><p><img src="/image/043.jpg" alt="043"></p><p>又回家了。</p><p><img src="/image/044.jpg" alt="044"></p><p>臭美先生。</p><p><img src="/image/045.jpg" alt="045"></p><p><img src="/image/046.jpg" alt="046"></p><p>回去看老师了，看看没咋来的校区，还有可爱的老牟。</p><p><img src="/image/047.jpg" alt="047"></p><p><img src="/image/048.jpg" alt="048"></p><p>哥们开车又去大理了。</p><p><img src="/image/049.jpg" alt="049"></p><p>刷完回来我就去手术了，割了好大一块息肉下来，已马赛克处理</p><p><img src="/image/050.jpg" alt="050"></p><p>药罐子卞卞，现在都喝了五个疗程了。悲</p><p><img src="/image/051.jpg" alt="051"></p><p>和黄某约会成功了。</p><p><img src="/image/052.jpg" alt="052"></p><p>工位回到链时代了，说实话，我到现在我都觉得自己很担心能不能搞好，我不认为我是个优秀的人，我一直害怕辜负别人的期望，我尽力吧。</p><p><img src="/image/053.jpg" alt="053"></p><p>和智航骑车，我又来看你了。</p><p><img src="/image/054.jpg" alt="054"></p><p>工位摸鱼看霉霉。</p><p><img src="/image/055.jpg" alt="055"></p><p>招新成功吃饭了，这下人真的躲多起来了。</p><p><img src="/image/056.jpg" alt="056"></p><p>没试过，还蛮好玩的。</p><p><img src="/image/057.jpg" alt="057"></p><p>天晴了，我是不是该出发了。</p><h1 id="最后碎碎念"><a href="#最后碎碎念" class="headerlink" title="最后碎碎念"></a>最后碎碎念</h1><p>1.说实话，我一年好像过的很丰富也很痛苦，放在一年前，我都不敢想我现在的样子，一堆事的我。不过我还是蛮开心的，内耗的开心，哈哈哈哈哈。我还是那个时不时会哭的孩子。</p><p>2.大二下主要还是比赛比较多，从我做完手术，拆线的第二天我就开始打比赛了，这学期真的比赛太磨人了。虽然现在看好几个比赛好像结果不是很好，但是我也就这样了，哎。</p><p>3.其实这一年认识的人蛮多的，我还是很开心，虽然其实真的，我在熟人面前很e，但是其实我比较i的，现在不是已经进化到绿老头了吗。我很开心认识大家，不管是哪里的，是坐在隔壁的，还是远在他乡的。</p><p>4.我心理状况还是那么差，好像我体验到历史最差了，不过还好，我至少学会求助了，我现在很强大。</p><hr><p><strong>二十岁了，发在十九岁最后一天，我好像还有很多路要走，好累，但是也没办法，我只能慢慢硬着头皮走下去了。</strong></p><p>最后，我想用张信哲的一首歌来结尾：</p><blockquote><p>我的整个世界<br>面目已全非<br>所有爱恨喜悲<br>都在天上飞</p><p>究竟还有甚么挂念让我不能睡<br>为何觉得如此的狼狈<br>承受不了你的善变我知难而退<br>不管对不对 是对爱不想有所违背</p><p>心情再乱 再坏 再无奈 也不掉一滴泪<br>心上的那个空缺不求人安慰<br>虽然寂寞如影相随 怎么都不对<br>只怕藕断丝连 伤痛会加倍</p></blockquote><p><strong>祝我，天天开心！！！！！！！</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/image/001.jpg&quot; alt=&quot;001&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前情提要：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;想想之前上次生日还是订了个好大的冰淇淋蛋糕，但是那天下雨，还挺大的，但是不方便我觉得真的好好吃&lt;/p&gt;
&lt;h1 id=&quot;202</summary>
      
    
    
    
    <category term="大学" scheme="http://example.com/categories/%E5%A4%A7%E5%AD%A6/"/>
    
    
    <category term="年度总结" scheme="http://example.com/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>SAM论文细读</title>
    <link href="http://example.com/undefined/5174.html"/>
    <id>http://example.com/undefined/5174.html</id>
    <published>2024-05-04T14:13:40.000Z</published>
    <updated>2024-05-05T14:07:45.061Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>论文地址：<a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a></p><p>项目地址：<a href="https://github.com/facebookresearch/segment-anything">Segment Anything</a></p><p>项目演示：<a href="https://segment-anything.com/demo">https://segment-anything.com/demo</a></p><h1 id="基本思想："><a href="#基本思想：" class="headerlink" title="基本思想："></a>基本思想：</h1><h2 id="1-零次学习："><a href="#1-零次学习：" class="headerlink" title="1.零次学习："></a>1.零次学习：</h2><p><img src="/image/1.PNG" alt="img"></p><p>下图所示：</p><p>利用过去的知识（马，老虎，熊猫和斑马的描述），在脑海中推理出新对象的具体形态，从而能对新对象进行辨认。就是希望能够模仿人类的这个推理过程，使得计算机具有识别新事物的能力。</p><p><img src="/image/2.PNG" alt="img"></p><h2 id="2-结构分析"><a href="#2-结构分析" class="headerlink" title="2.结构分析"></a>2.结构分析</h2><p><img src="/image/3.PNG" alt="img"></p><p>SAM模型大致上分成3个模块，一个标准的vit构成的image encoder、一个prompt encoder和一个mask decoder。其中：</p><ul><li>Image encoder： 用于输出image embedding；</li><li>prompt encoder：用于接收point、box、txt的编码信息，并且与image embedding组合到一起送入mask decoder中；</li><li>mask decoder：将上述两个encoder的编码信息转化为mask输出。</li></ul><h3 id="一、image-encoder"><a href="#一、image-encoder" class="headerlink" title="一、image encoder"></a>一、image encoder</h3><h4 id="1-、复习一下"><a href="#1-、复习一下" class="headerlink" title="(1)、复习一下"></a>(1)、复习一下</h4><p>先复习一下关于vit的东西。</p><p><img src="/image/4.gif" alt="img"></p><p><img src="/image/5.PNG" alt="img"></p><p>模型由三个模块组成：</p><ul><li>Linear Projection of Flattened Patches(Embedding层)</li><li>Transformer Encoder</li><li>MLP Head（最终用于分类的层结构）</li></ul><p>工作流程如下：</p><p>ViT的工作流程，如下：</p><ol><li>将一张图片分成patches</li><li>将patches铺平</li><li>将铺平后的patches的线性映射到更低维的空间</li><li>添加位置embedding编码信息</li><li>将图像序列数据送入标准Transformer encoder中去</li><li>在较大的数据集上预训练</li><li>在下游数据集上微调用于图像分类</li></ol><p>步骤1、将图片转换成patches序列</p><p>这一步很关键，为了让Transformer能够处理图像数据，第一步必须先将图像数据转换成序列数据，但是怎么做呢？假如我们有一张图片，patch大小为，那么我们可以创建个图像patches，可以表示为，其中，就是序列的长度，类似一个句子中单词的个数。在上面的图中，可以看到图片被分为了9个patches。</p><p>步骤2、将Patches铺平</p><p>在原论文中，作者选用的patch大小为16，那么一个patch的shape为(3,16,16)，维度为3，将它铺平之后大小为3x16x16&#x3D;768。即一个patch变为长度为768的向量。不过这看起来还是有点大，此时可以使用加一个Linear transformation，即添加一个线性映射层，将patch的维度映射到我们指定的embedding的维度，这样就和NLP中的词向量类似了。</p><p>步骤3、添加Position embedding</p><p>与CNNs不同，此时模型并不知道序列数据中的patches的位置信息。所以这些patches必须先追加一个位置信息，也就是图中的带数字的向量。实验表明，不同的位置编码embedding对最终的结果影响不大，在Transformer原论文中使用的是固定位置编码，在ViT中使用的可学习的位置embedding 向量，将它们加到对应的输出patch embeddings上。</p><p>步骤4、添加class token</p><p>在输入到Transformer Encoder之前，还需要添加一个特殊的class token，这一点主要是借鉴了BERT模型。添加这个class token的目的是因为，ViT模型将这个class token在Transformer Encoder的输出当做是模型对输入图片的编码特征，用于后续输入MLP模块中与图片label进行loss计算。</p><p>步骤5、输入Transformer Encoder</p><p>将patch embedding和class token拼接起来输入标准的Transformer Encoder中。 Transformer Encoder其实就是重复堆叠Encoder Block L次，主要由Layer Norm、Multi-Head Attention、Dropout和MLP Block几部分组成。</p><p>步骤6、分类</p><p>注意Transformer Encoder的输出其实也是一个序列，但是在ViT模型中只使用了class token的输出，将其送入MLP模块中，去输出最终的分类结果。</p><p><strong>Embedding层</strong></p><p>对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]，如下图，token0-9对应的都是向量，以ViT-B&#x2F;16为例，每个token向量长度为768。</p><p><img src="/image/6.png" alt="img"></p><p>对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。首先将一张图片按给定大小分成一堆Patches。以ViT-B&#x2F;16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后会得到196个Patches。接着通过线性映射将每个Patch映射到一维向量中，以ViT-B&#x2F;16为例，每个Patche数据shape为[16, 16, 3]通过映射得到一个长度为768的向量（后面都直接称为token）。[16, 16, 3] -&gt; [768]</p><p>在代码实现中，直接通过一个卷积层来实现。 以ViT-B&#x2F;16为例，直接使用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积来实现。通过卷积[224, 224, 3] -&gt; [14, 14, 768]，然后把H以及W两个维度展平即可[14, 14, 768] -&gt; [196, 768]，此时正好变成了一个二维矩阵，正是Transformer想要的。</p><p><img src="/image/7.png" alt="img"></p><p>这里面就是把以前基于NLP的tansformer改成为图像的可输入，图像也变成二维矩阵。</p><p><strong>Transformer Encoder</strong></p><p>Transformer Encoder其实就是重复堆叠Encoder Block L次，主要由Layer Norm、Multi-Head Attention、Dropout和MLP Block几部分组成。</p><p><img src="/image/8.png" alt="img"></p><p><strong>MLP Head</strong></p><p>上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的，以ViT-B&#x2F;16为例，输入的是[197, 768]输出的还是[197, 768]。这里我们只是需要分类的信息，所以我们只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。MLP Head原论文中说在训练ImageNet21K时是由Linear+tanh激活函数+Linear组成。但是迁移到ImageNet1K上或者你自己的数据上时，只用一个Linear即可。</p><p><img src="/image/9.png" alt="img"></p><p><strong>参数说明</strong></p><p><img src="/image/10.png" alt="img"></p><h4 id="2-、结构说明"><a href="#2-、结构说明" class="headerlink" title="(2)、结构说明"></a>(2)、结构说明</h4><ul><li>输入图像进入网络，先经过一个卷积base的patch_embedding：取16*16为一个patch，步长也是16，这样feature map的尺寸就缩小了16倍，同时channel从3映射到768。</li><li>patch_embed过后加positional_embedding：positional_embedding是个可学习的参数矩阵，初始化是0。</li><li>加了positional_embedding后的feature map过16个transformer block，其中12个transformer是基于window partition（就是把特征图分成14*14的windows做局部的attention）的attn模块，和4个全局attn，这4个全局attn是穿插在windowed attention中的。</li><li>最后过两层卷积（neck）把channel数降到256，这就是最终的image embedding的结果</li></ul><p>从结构上看，sam的encoder部分就是堆叠transformer的block结构，最后再跟一个neck，调整输出embedding的维度。Meta开源了三个模型，分别是vit_h, vit_l和vit_b，这三个模型的区别仅仅在于内部patch embedding维度、transformer的block的个数以及每个block中head的数量和全局attention的index：</p><p><img src="/image/11.png" alt="img"></p><p><strong>1.1 图片分patch</strong></p><p>原图进入网络之后，按照最大边长补充成方形，再resize到1024x1024。</p><p>1024x1024x3的图片输入进入网络后，首先使用一个16x16，stride&#x3D;16，输出channel数为patch embedding维度的二维卷积。以vit_b为例，patch embedding的维度是768，因此经过卷积之后，图片变成了768x64x64的feature map，再调整维度就变成64x64x768。</p><p>在该feature map基础上，会再加一个绝对位置编码(absolute positional embedding)，所谓绝对位置编码是指生成一组与feature map同样大小(64x64x768)的可学习参数，初始化时一般为0。</p><p><strong>1.2 attention block</strong></p><p><img src="/image/12.png" alt="img"></p><p><img src="/image/13.png" alt="img"></p><p><img src="/image/14.png" alt="img"></p><p><img src="/image/15.png" alt="img"></p><p><img src="/image/16.png" alt="img"></p><h2 id="二、-Prompt-encoder"><a href="#二、-Prompt-encoder" class="headerlink" title="二、 Prompt encoder"></a>二、 Prompt encoder</h2><p>根据输入的point和boxs返回sparse embedding， 根据mask返回dense embeddings。</p><p>PromptEncoder属于轻量化的结构，用于对输入模型的points、boxes和masks信息进行编码，将其统一为空间特征编码的格式。其对points、boxes和masks编码时允许有部分值空缺（空缺使用默认值），其将points和boxes组装为sparse_embeddings，将mask组装为dense_embeddings 其对mask的采样由多个attention层实现，具体可见mask_downscaling函数</p><h3 id="point-embedding"><a href="#point-embedding" class="headerlink" title="point embedding"></a>point embedding</h3><ul><li><strong>step1：首先生成一组可学习的向量point embedding，大小为：4x1x256：</strong></li></ul><p>4代表了表示pos&#x2F;neg + 2 box corners，即demo里面的添加点和消除点、以及box框的左上角和右下角；</p><p>0：neg，对应demo中的消除点</p><p>1：pos，对应demo中的添加点</p><p>2：代表box左上角点</p><p>3：代表box右下角点</p><ul><li><strong>step2：再生成一组可学习的向量not_a_point_embed，大小为1x256，用于表示该位置不是一个点</strong></li><li><strong>step3：如果传入的prompt里面没有bbox，则补充一个【0，0】点到每个point后面，其对应的label为-1</strong></li><li><strong>step4：如果传入的还有bbox，此时的point大小为Nx1x2，label为Nx1</strong></li><li><strong>step5：再根据point计算point embedding，其流程如下：</strong></li></ul><p>横纵坐标先归一化，即都除以输入的尺寸(1024, 1024)；</p><p>再将point矩阵与一个随机高斯矩阵(2x128)矩阵相乘得到Nxax128的矩阵coord，其中（a&#x3D;2表示只有point，a&#x3D;1表示还有box作为prompt输入）；</p><p>再分别对coord计算sin和cos，拼接矩阵得到最终的point embedding（Nxax256）</p><ul><li><strong>step：6再根据label，给point embedding加上之前生成的可学习的embeding向量</strong></li></ul><p><img src="/image/17.png" alt="img"></p><h3 id="box-embedding"><a href="#box-embedding" class="headerlink" title="box embedding"></a>box embedding</h3><p>bbox一般有2个点，其编码步骤如下：</p><p>step1： 和point一样，先四个点resize为Nx2x2；</p><p>step2： 再使用point embedding编码的方式，得到corner_embedding，</p><p>step3： 再加上之前生成的可学习的embeding向量；</p><p>最后输出的corner_embedding大小为Nx2x256。</p><p>最后输出的box的embedding的尺寸是Nx2x256。</p><p>合并（concat）point embedding和corner embedding，可以得到sparse embedding：</p><p>全都没有：sparse embedding（1x0x256）</p><p>如果只有point：sparse embedding（Nx2x256）</p><p>如果只有box：sparse embedding（Nx2x256）</p><p>piont、box都有：sparse embedding（Nx3x256）</p><h3 id="mask-embedding"><a href="#mask-embedding" class="headerlink" title="mask embedding"></a>mask embedding</h3><p>那么对于mask这类的dense prompt，他的映射就比较简单粗暴。在输入prompt encoder之前，先要把mask降采样到4x，再过两个2x2，stride&#x3D;2的卷积，这样尺寸又降了4x，就和降了16x的图像特征图尺寸一致了，再过一个1*1的卷积，把channel也升到256。如果没有提供mask，也就是我们实际inference时候的场景，这个结构会直接返回一个描述“没有mask”特征的特征图</p><h2 id="三、mask-decoder"><a href="#三、mask-decoder" class="headerlink" title="三、mask decoder"></a>三、mask decoder</h2><p><img src="/image/18.png" alt="img"></p><p><img src="/image/19.png" alt="img"></p><p>decoder的结构之所以看起来复杂，主要原因是prompt embedding和image embedding在这个结构中反复融合并且反复更新，从这里同样可以看出prompt在这个任务中的重要地位。</p><p>我们从左至右逐步分析decoder的流程，</p><ul><li>在prompt embedding进入decoder之前，先在它上面concat了一组可学习的output tokens，output tokens由两个部分构成：<ul><li>一个是iou token，它会在后面被分离出来用于预测iou的可靠性（对应结构图右侧的IoU output token），它受到模型计算出的iou与模型计算出的mask与GT实际的iou之间的MSE loss监督；</li><li>另一个是mask token，它也会在后面被分离出来参与预测最终的mask（对应结构图右侧的output token per mask），mask受到focal loss和dice loss 20:1的加权组合监督。</li><li>这两个token的意义我感觉比较抽象，因为理论来说进入decoder的变量应该是由模型的输入，也就是prompt和image的映射构成，但这两个token的定义与prompt和image完全没有关系，而是凭空出现的。从结果反推原因，只能把它们理解成对模型的额外约束，因为它们两个参与构成了模型的两个输出并且有loss对他们进行监督。</li><li>最终prompt embedding（这一步改名叫prompt token）和刚才提到这两个token concat到一起统称为tokens进入decoder。</li></ul></li><li>image embedding在进入decoder之前也要进行一步操作：dense prompt由于包含密集的空间信息，与image embedding所在的特征空间一致性更高，所以直接与image embedding相加融合。因为后面要与prompt做cross attention融合，这里还要先算一下image embedding的位置编码。</li><li>接下来{image embedding，image embedding的位置编码，tokens}进入一个两层transformer结构的decoder做融合。值得注意的是，在transformer结构中，为了保持位置信息始终不丢失，每做一次attention运算，不管是self-attention还是cross-attention，tokens都叠加一次初始的tokens，image embedding都叠加一次它自己的位置编码，并且每个attention后边都接一个layer_norm。<ul><li>tokens先过一个self-attention。</li><li>tokens作为q，对image embedding做cross attention，更新tokens。</li><li>tokens再过两层的mlp做特征变换。</li><li>image embedding作为q，对tokens做cross attention，更新image embedding。</li></ul></li><li>更新后的tokens作为q，再对更新后的image embedding做cross attention，产生最终的tokens。</li><li>更新后的image embedding过两层kernel_size&#x3D;2, stride&#x3D;2的转置卷积，升采样到4x大小（依然是4x降采样原图的大小），产生最终的image embedding。</li><li>接下来兵分两路：<ul><li>mask token被从tokens中分离出来（因为他一开始就是concat上去的，可以直接按维度摘出来），过一个三层的mlp调整channel数与最终的image embedding一致，并且他们两个做矩阵乘法生成mask的预测。</li><li>iou token被从tokens中分离出来，也过一个三层的mlp生成最终的iou预测。</li></ul></li><li>最后，如前文所述，分别对mask的预测和iou预测进行监督，反向传播，更新参数。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2304.02643&quot;&gt;https://arxiv.org/abs/2304.02643&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/fac</summary>
      
    
    
    
    <category term="科研" scheme="http://example.com/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="医学图像" scheme="http://example.com/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"/>
    
  </entry>
  
</feed>
